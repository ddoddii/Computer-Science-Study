### Operating Systems

<details>
<summary>📚 공부한 자료</summary>

- 혼자 공부하는 컴퓨터구조 / 운영체제
- 실습과 그림으로 배우는 리눅스 구조
- Operating System Concepts

</details>

### **1. 시스템 콜이 무엇인지 설명해 주세요. ✅**

시스템 콜이란 사용자 모드로 실행되는 프로그램이 자원에 접근하는 운영체제 서비스를 제공받기 위해 **운영체제의 커널 기능을 사용하도록 요청**하는 것이다. 시스템콜은 일종의 소프트웨어적 인터럽트이다.

- **우리가 사용하는 시스템 콜의 예시를 들어주세요.**
  - 파일 관련: `open()`, `read()`, `write()`, `close()`
  - 프로세스 관련: `fork()`, `exec()`, `wait()`, `exit()`
  - 통신 관련: `socket()`, `bind()`, `listen()`, `accept()`
- **시스템 콜이, 운영체제에서 어떤 과정으로 실행되는지 설명해 주세요.**
  1. **시스템 콜 요청:** 사용자 프로그램이 시스템 콜을 요청합니다.
  2. **컨텍스트 스위칭:** 프로그램은 사용자 모드에서 커널 모드로 전환합니다.
  3. **커널에서 처리:** 운영체제의 커널이 요청받은 시스템 콜을 처리합니다.
  4. **컨텍스트 복원:** 처리가 완료되면 다시 사용자 모드로 돌아갑니다.
- **시스템 콜의 유형에 대해 설명해 주세요.**
  - **프로세스 제어:** 프로세스 생성, 종료, 중단 등을 관리합니다.
  - **파일 조작:** 파일 생성, 삭제, 읽기, 쓰기 등의 작업을 수행합니다.
  - **장치 관리:** 장치에 대한 요청과 해제, 장치 속성 읽기 등을 다룹니다.
  - **정보 유지:** 시간, 날짜, 시스템 데이터 설정과 조회 등을 합니다.
  - **통신:** 프로세스 간 통신이나 네트워크 통신을 관리합니다.
- **운영체제의 Dual Mode 에 대해 설명해 주세요.**
  - **유저 모드(User Mode):** 사용자 프로그램이 실행되는 보통의 모드입니다. 제한된 접근 권한을 가지고 있습니다.
  - **커널 모드(Kernel Mode):** 운영체제가 시스템 하드웨어와 직접 상호작용할 때 사용하는 모드입니다. 모든 메모리와 하드웨어에 대한 접근이 가능합니다.
- **왜 유저모드와 커널모드를 구분해야 하나요?**
  - **보안:** 사용자 프로그램이 시스템의 중요한 부분을 잘못 조작하는 것을 방지합니다. 만약 모든 응용 프로그램이 CPU, 메모리, 하드 디스크 등에 마음대로 접근하고 조작할 수 있다면 자원이 무질서하게 관리되고 조금만 실수해도 컴퓨터 전체에 악영향을 끼친다.
  - **안정성:** 시스템의 안정적인 운영을 보장하기 위해, 사용자의 임의적인 하드웨어 접근을 제한합니다.
  - **자원 관리:** 시스템 자원을 효율적으로 관리하고, 프로그램 간 간섭을 방지합니다.
- **서로 다른 시스템 콜을 어떻게 구분할 수 있을까요?**

  - 서로 다른 시스템 콜(System Calls)을 구분하는 주요 방법은 **시스템 콜 식별자(System Call Identifiers)** 또는 **번호(System Call Numbers)**를 사용하는 것입니다. 이 식별자들은 운영체제가 제공하는 각각의 시스템 콜에 고유한 숫자를 할당하여 구분합니다.

### **2. 인터럽트가 무엇인지 설명해 주세요. ✅**

인터럽트는 CPU 의 작업을 방해하는 신호입니다. 인터럽트의 종류에는 크게 동기 인터럽트 / 비동기 인터럽트가 있다. 동기 인터럽트(=예외) - CPU 에 의해 발생하는 인터럽트, 비동기 인터럽트(=하드웨어 인터럽트) - 입출력장치에 의해 발생.

- **인터럽트는 어떻게 처리하나요?**
  - 1. 입출력장치 → CPU 인터럽트 요청 신호 보냄
  - 2. CPU 는 실행 사이클이 끝나고 명령어를 인출하기 전 항상 인터럽트 여부 확인
  - 3. CPU 는 인터럽트 요청을 확인하고 인터럽트 플래그를 통해 현재 인터럽트를 받아들일 수 있는지 여부 확인
  - 4. 인터럽트를 받아들일 수 있다면 CPU 는 지금까지의 작업 백업
  - 5. CPU 는 인터럽트 벡터를 참조하여 인터럽트 서비스 루틴(ISR)을 실행
  - 6. 인터럽트 서비스 루틴이 끝나면 백업해 둔 작업 복구
- **Polling 방식에 대해 설명해 주세요.**
  - CPU가 주기적으로 각 장치의 상태를 체크하여 서비스가 필요한지 확인. 인터럽트와 대비되는 방법. CPU가 지속적으로 장치들을 검사하여 요청을 확인하고, 요청이 있을 떄 요청을 처리함. (ex. 키보드 컨트롤러)
- **HW / SW 인터럽트에 대해 설명해 주세요.**
  - 하드웨어 인터럽트:
    - **발생 원인:** 외부 하드웨어 장치로부터 발생합니다 (예: 키보드 입력, 마우스 클릭).
    - **처리:** CPU는 하드웨어 인터럽트를 받고, 관련 인터럽트 서비스 루틴을 실행합니다.
  - 소프트웨어 인터럽트(=예외):
    - **발생 원인:** 소프트웨어 명령어에 의해 발생합니다 (예: 시스템 호출).
    - **처리:** 프로그램 코드 내에서 명시적으로 발생시키며, CPU는 이를 처리하기 위해 ISR을 실행합니다.
- **동시에 두 개 이상의 인터럽트가 발생하면, 어떻게 처리해야 하나요?**
  - **우선 순위 부여:** 인터럽트에는 우선 순위가 있어, 높은 우선 순위의 인터럽트를 먼저 처리합니다.
  - **중첩 인터럽트(Nested Interrupt):** 높은 우선 순위의 인터럽트 처리 중 다른 인터럽트가 발생하면, 현재 인터럽트 처리를 중단하고 새 인터럽트를 처리합니다.
  - 하드웨어 인터럽트 처리 → 컴퓨터에서는 Programmable Interrupt Controller (PIC)라는 하드웨어를 사용한다. PIC 는 여러 장치 컨트롤러에서 보낸 하드웨어 인터럽트 요청들의 우선순위를 판별한 뒤 CPU 에 지금 처리해야 하는 인터럽트가 무엇인지 알려준다.

### **3. 프로세스가 무엇인가요?. ✅**

프로세스는 실행 중인 프로그램의 인스턴스이다. 운영체제에 의해 메모리와 필요한 리소스가 할당되고, 최소 하나의 스레드를 포함합니다. 프로세스는 자신만의 독립된 메모리 영역을 가지며, 다른 프로세스와 리소스를 공유하지 않습니다.
프로세스의 현재 활동의 상태는 프로그램 카운터 값과 프로세스 레지스터의 내용으로 나타낸다.

- **프로그램과 프로세스, 스레드의 차이에 대해 설명해 주세요.**
  - 프로그램 : 디스크에 저장된 실행 가능한 코드의 모음입니다. 실행되지 않는 정적인 상태입니다.
  - 프로세스(process) : 프로그램이 실행되어 메모리에 올라간 상태입니다. 독립된 메모리 영역과 시스템 리소스를 가지고, 운영체제에 의해 관리됩니다.
  - 스레드(thread) : 프로세스 내에서 실행되는 독립적인 실행 흐름입니다. 스레드는 프로세스의 리소스를 공유하면서 각기 다른 작업을 수행할 수 있습니다.
- **프로세스 상태는 어떤 것이 있나요?**
  - new : 프로세스가 생성 중
  - running : 명령어들이 실행 중
  - waiting : 프로세스가 어떤 이벤트를 기다림
  - ready : 프로세스가 처리기에 할당되기를 기다림
  - terminated : 프로세스의 실행이 종료됨
- **PCB가 무엇인가요?**
  - Process Control Block
  - PCB는 특정 프로세스에 대한 중요한 정보를 저장하는 운영체제의 데이터 구조입니다. PCB 는 커널 영역에 생성됩니다.
    - PID (Process ID) : 특정 프로세스를 식별하기 위한 고유한 번호
    - 프로그램 카운터 : 이 프로세스가 다음에 실행할 명령어 주소 → 인터럽트 발생 시 저장
    - CPU 레지스터 값 : 해당 프로세스가 실행하며 사용했던 레지스터 값 → 인터럽트 발생 시 저장
    - 프로세스 상태 : 프로세스의 현재 상태 (예: 실행 중, 준비 상태, 대기 상태, 종료 상태)
    - CPU 스케쥴링 정보: 프로세스 우선순위, 스케줄링 큐에 대한 정보 등 스케줄링 관련 정보를 포함합니다.
    - 메모리 관리 정보 : 프로세스가 메모리의 어느 주소에 저장되어 있는지. (베이스 레지스터 , 한계 레지스터), 페이지 테이블 정보
    - 사용한 파일과 입출력장치 목록
- **그렇다면, 스레드는 PCB를 갖고 있을까요?**
  - 스레드는 전통적인 의미에서 개별적인 프로세스 제어 블록(Process Control Block, PCB)을 가지고 있지 않습니다. 대신, 스레드는 프로세스의 PCB를 공유하면서도, 각 스레드의 고유한 실행 상태를 관리하기 위한 별도의 정보를 가지고 있습니다.
  - **스레드와 PCB**
    1. **PCB 공유:** 스레드는 동일한 프로세스에 속하므로 프로세스의 PCB를 공유합니다. PCB에는 프로세스의 전반적인 정보가 저장되어 있습니다(예: 프로세스 ID, 프로세스 상태, 메모리 정보 등).
    2. **스레드 별 정보:** 각 스레드는 고유한 스택, 스레드 ID, 스레드 상태, 레지스터 세트, 우선순위 등의 정보를 관리합니다. 이러한 정보는 프로세스의 PCB에 포함되어 있거나, 별도의 스레드 테이블 또는 스레드 제어 블록(Thread Control Block, TCB)에 저장될 수 있습니다.
  - **스레드 관리 방식**
    - **경량 프로세스(Lightweight Process, LWP):** 일부 운영체제에서는 스레드를 경량 프로세스로 취급하며, 각 스레드에 대한 정보를 관리하는 별도의 구조를 사용합니다.
    - **운영체제의 구현:** 스레드의 관리 방식은 운영체제마다 다를 수 있으며, 각 운영체제는 스레드의 상태와 특성을 추적 및 관리하기 위한 자체적인 메커니즘을 가지고 있습니다.
- **리눅스에서, 프로세스와 스레드는 각각 어떻게 생성될까요?**
  - **프로세스 생성**
    리눅스에서 새로운 프로세스는 주로 **`fork()`** 시스템 콜을 사용하여 생성됩니다.
    1. **`fork()`:** 이 시스템 콜은 호출하는 현재 프로세스(부모 프로세스)의 복사본(자식 프로세스)을 생성합니다. 이 복사본은 메모리 상태, 실행된 파일의 세그먼트, 열린 파일 디스크립터 등 부모 프로세스의 거의 모든 특성을 복제합니다.
    2. **`exec()`:** 자식 프로세스는 종종 **`exec()`** 계열의 시스템 콜을 통해 새로운 프로그램을 실행합니다. **`exec()`**는 호출한 프로세스의 메모리 영역에 새로운 프로그램을 로드하여 실행합니다.
  - **스레드 생성**
    리눅스에서 스레드(경량 프로세스라고도 불림)는 **`clone()`** 시스템 콜을 사용하여 생성됩니다. 최신 버전의 리눅스에서는 **`pthread_create()`** 라이브러리 함수를 통해 스레드를 생성하는 것이 일반적입니다.
    1. **`clone()`:** **`clone()`** 시스템 콜은 **`fork()`**와 유사하지만, 호출 프로세스의 특정 부분만을 복제할 수 있는 옵션을 제공합니다. 스레드 생성 시, **`clone()`**은 메모리 공간(코드, 데이터, 힙)을 공유하되 각 스레드에 독립적인 스택을 제공합니다.
    2. **`pthread_create()`:** POSIX 스레드 라이브러리는 **`pthread_create()`** 함수를 제공하여 스레드를 생성하고 관리합니다. 이 함수는 내부적으로 **`clone()`**을 사용하여 리눅스 스레드를 생성합니다.
  - **공유와 독립**
    - 프로세스는 독립적인 메모리 공간과 실행 컨텍스트를 가지지만, 스레드는 프로세스 내에서 실행 컨텍스트를 공유합니다.
    - 스레드는 메모리와 자원을 공유하기 때문에 프로세스보다 생성과 컨텍스트 스위칭이 가볍고 빠릅니다.
- **자식 프로세스가 상태를 알리지 않고 죽거나, 부모 프로세스가 먼저 죽게 되면 어떻게 처리하나요?**
  - **자식 프로세스가 상태를 알리지 않고 종료되는 경우**
    1. **좀비 프로세스(Zombie Process):** 자식 프로세스가 종료될 때, 그 상태 정보는 운영체제에 의해 임시 저장됩니다. 만약 부모 프로세스가 **`wait()`** 시스템 콜을 사용해 자식 프로세스의 종료 상태를 회수하지 않는다면, 자식 프로세스는 좀비 프로세스가 됩니다. 좀비 프로세스는 프로세스 테이블에서 자원을 해제하지 않은 상태로 남아 있게 되며, 이는 시스템 리소스 낭비로 이어질 수 있습니다.
    2. **부모 프로세스의 책임:** 일반적으로 부모 프로세스는 자식 프로세스의 종료 상태를 회수하기 위해 **`wait()`** 또는 **`waitpid()`** 함수를 호출하는 것이 좋습니다.
  - **부모 프로세스가 먼저 종료되는 경우**
    1. **고아 프로세스(Orphan Process):** 부모 프로세스가 종료되고 자식 프로세스가 계속 실행 중인 경우, 자식 프로세스는 고아 프로세스가 됩니다.
    2. **`init` 프로세스의 책임:** 리눅스와 같은 대부분의 유닉스 계열 운영체제에서는 고아 프로세스가 **`init`** 프로세스(프로세스 ID가 1인 프로세스)의 자식이 되어, **`init`** 프로세스가 자식 프로세스의 종료 상태를 회수합니다. **`init`** 프로세스는 시스템 부팅 시에 시작되며 시스템이 종료될 때까지 계속 실행됩니다.
- **리눅스에서, 데몬프로세스에 대해 설명해 주세요.**
  - 리눅스에서 데몬 프로세스(Daemon Process)는 백그라운드에서 실행되는 특별한 종류의 프로세스를 말합니다. 데몬은 사용자의 직접적인 개입 없이 특정 서비스를 제공하기 위해 운영체제에 의해 자동으로 실행되며, 시스템 부팅 시에 시작되어 시스템이 종료될 때까지 계속 실행되는 경우가 일반적입니다.
  - **데몬 프로세스의 특징**
    1. **백그라운드 실행:** 데몬은 백그라운드에서 실행되므로, 사용자 인터페이스를 갖지 않습니다. 이들은 사용자와의 직접적인 상호작용 없이 서비스를 제공합니다.
    2. **장기 실행:** 대부분의 데몬은 시스템이 부팅될 때 시작되어 시스템이 종료될 때까지 계속 실행됩니다.
    3. **자동 시작:** 데몬은 대개 시스템의 초기화 스크립트나 서비스 매니저에 의해 자동으로 시작됩니다.
    4. **시스템 서비스 제공:** 데몬은 파일 시스템 모니터링, 로그 파일 관리, 네트워크 서비스, 하드웨어 상호작용 등 다양한 시스템 수준의 서비스를 제공합니다.
  - **데몬 프로세스의 예**
    - **`httpd`:** 웹 서버를 위한 데몬으로, HTTP 요청을 처리합니다.
    - **`sshd`:** SSH 연결을 관리하는 데몬입니다.
    - **`crond`:** 예약된 작업을 정기적으로 실행하는 데몬입니다.
- **리눅스는 프로세스가 일종의 트리를 형성하고 있습니다. 이 트리의 루트 노드에 위치하는 프로세스에 대해 설명해 주세요.**
  - **Init 프로세스**
    1. **프로세스 ID (PID):** **`init`** 프로세스는 대부분의 유닉스 및 리눅스 시스템에서 PID 1을 가집니다. 이는 시스템 부팅 과정에서 가장 먼저 시작되는 프로세스입니다.
    2. **시스템 초기화:** **`init`** 프로세스는 시스템의 초기화를 담당합니다. 이는 다른 필수 시스템 서비스 및 데몬 프로세스를 시작하는 역할을 포함합니다.
    3. **고아 프로세스의 부모:** **`init`** 프로세스는 고아 프로세스(orphan processes)의 부모 프로세스 역할을 합니다. 즉, 부모 프로세스가 종료되었을 때 자식 프로세스를 수용하여 그 상태를 관리합니다.
    4. **시스템 종료 관리:** 시스템의 종료 및 재부팅 과정을 관리합니다. **`init`** 프로세스가 시스템 종료 명령을 받으면, 다른 프로세스들을 안전하게 종료시키는 작업을 수행합니다.
  - **systemd**
    최근 많은 리눅스 배포판에서는 **`systemd`**가 **`init`** 프로세스의 역할을 대체하고 있습니다. **`systemd`**는 보다 현대적이고 효율적인 시스템 및 서비스 관리자로, **`init`**에 비해 더 많은 기능과 더 빠른 부팅 시간을 제공합니다. **`systemd`** 역시 PID 1을 가지며, 시스템의 첫 번째 프로세스로서의 역할을 수행합니다.

### **4. 프로세스 주소공간에 대해 설명해 주세요. ✅**

프로세스가 생성되면 커널 영역에 PCB 가 생성된다. 사용자 영역에는 크게 코드 영역, 데이터 영역, 힙 영역, 스택 영역으로 나뉘어 저장된다. (코드영역, 데이터 영역) → 정적 할당 영역 / (힙 영역, 스택 영역) → 동적 할당 영역

- **프로세스의 4가지 영역은 무엇인가요?**
  - code segment : 실행할 수 있는 코드 (기계어로 이루어진 명령어) → 읽기 전용
  - data segment : 프로그램이 실행되는 동안 유지할 데이터(전역 변수)가 저장된다. 2가지 부분으로 나뉜다.
    - 초기화된 data segment : 프로그래머에 의해 초기값이 할당된 전역 변수 및 정적 변수를 포함합니다.
    - **BSS (Block Started by Symbol) 세그먼트:** 초기화되지 않은 전역 변수 및 정적 변수를 포함합니다. 이 변수들은 프로그램 시작 시 0 또는 null로 초기화됩니다.
  - heap segment: 프로그램을 만드는 사용자가 직접 할당할 수 있는 저장 공간. 반환하지 않으면 메모리 누수가 발생. 프로그램 실행 중에 동적으로 할당되는 메모리.
  - stack segment: 함수를 호출할 때 임시 데이터 저장 공간. (매개 변수, 지역 변수, 복귀 주소)
- **초기화 하지 않은 변수들은 어디에 저장될까요?**
  - 운영체제의 프로세스 주소 공간에서 초기화되지 않은 변수들은 주로 "BSS 세그먼트" (Block Started by Symbol)에 위치합니다.
  - **초기화:** BSS 세그먼트에 있는 변수들은 프로그램 시작 시 0 또는 null로 자동 초기화됩니다.
  - **메모리 효율성:** BSS 세그먼트에 있는 변수들은 실제로 메모리에 할당되기 전까지는 어떤 실제 메모리 공간도 차지하지 않습니다. 이는 'Lazy Allocation'이라는 메모리 할당 방식 때문입니다.
  - **용도:** 전역 변수나 정적 변수 중에서 프로그램에 의해 명시적으로 초기화되지 않은 변수들을 저장하기 위해 사용됩니다.
- **일반적인 주소공간 그림처럼, Stack과 Heap의 크기는 매우 크다고 할 수 있을까요? 그렇지 않다면, 그 크기는 언제 결정될까요?**
  - **스택의 크기**
    1. **초기 크기:** 스택의 초기 크기는 운영체제나 프로그래밍 언어의 컴파일러 설정에 따라 결정됩니다. 예를 들어, 많은 운영체제에서는 각 스레드의 스택 크기를 기본값으로 설정합니다.
    2. **최대 크기:** 스택의 최대 크기 역시 운영체제나 프로그램 실행 환경 설정에 의해 정해집니다. 일부 시스템에서는 개발자가 스택의 최대 크기를 조절할 수 있는 옵션이 있습니다.
    3. **동적 확장:** 일반적으로 스택은 정적으로 할당되며, 크기가 너무 커지면 스택 오버플로우(stack overflow) 오류가 발생할 수 있습니다.
  - **힙의 크기**
    1. **초기 크기:** 힙의 초기 크기는 프로그램이 시작할 때 운영체제에 의해 할당됩니다.
    2. **동적 할당:** 힙은 동적 메모리 할당을 위한 영역이므로, 프로그램 실행 중 메모리 요청에 따라 그 크기가 늘어날 수 있습니다.
    3. **가용 메모리에 따라 달라짐:** 힙의 최대 크기는 시스템의 가용 메모리와 운영체제의 메모리 관리 정책에 따라 달라집니다.
- **Stack과 Heap 공간에 대해, 접근 속도가 더 빠른 공간은 어디일까요?**
  - **스택의 접근 속도**
    1. **구조적 단순성:** 스택은 LIFO (Last In, First Out) 구조로, 가장 최근에 들어간 데이터가 가장 먼저 나옵니다. 이 단순한 접근 방식은 메모리 관리를 간단하게 만듭니다.
    2. **데이터 국소성:** 스택은 함수 호출과 지역 변수에 사용되므로, 스택에 있는 데이터는 일반적으로 시간적 및 공간적 국소성이 높습니다. 이는 CPU 캐시에서 데이터가 더 자주 적중될 가능성을 의미합니다.
    3. **메모리 할당 속도:** 스택 메모리는 컴파일 시간에 결정되며, 실행 시간에는 데이터를 Push하거나 Pop하는 것이 전부이므로 메모리 할당과 해제가 매우 빠릅니다.
  - **힙의 접근 속도**
    1. **동적 할당의 복잡성:** 힙은 동적 메모리 할당을 위해 사용되며, 메모리 할당과 해제가 스택에 비해 복잡합니다. 이로 인해 오버헤드가 발생할 수 있습니다.
    2. **단편화:** 힙에서는 메모리 단편화가 발생할 수 있으며, 이는 메모리 할당의 효율성을 감소시키고 접근 속도를 늦출 수 있습니다.
    3. **데이터 국소성 부족:** 힙에 저장된 데이터는 스택에 비해 시간적 및 공간적 국소성이 낮을 수 있어, 캐시 적중률이 상대적으로 낮을 수 있습니다.
- **다음과 같이 공간을 분할하는 이유가 있을까요?**
  - 이러한 분할은 메모리 관리, 보안, 효율성, 그리고 프로그램의 성능 최적화에 중요한 역할을 합니다.
    **1. 메모리 관리**
  - **효율적 할당:** 힙과 스택은 서로 다른 메모리 할당 요구 사항을 충족합니다. 스택은 컴파일 타임에 크기가 결정되며, 함수 호출과 반환에 사용됩니다. 반면, 힙은 동적으로 할당되며, 프로그램 실행 중에 크기가 변할 수 있습니다.
  - **자동 메모리 관리:** 스택은 자동으로 관리되는 LIFO 구조를 가지고 있어, 함수 호출과 반환 시 지역 변수의 할당과 해제가 자동으로 이루어집니다.
    **2. 보안과 안정성**
  - **데이터 격리:** 코드, 데이터, 힙, 스택을 분리함으로써 서로 다른 메모리 영역 간의 데이터 침범을 방지합니다. 이는 프로그램의 안정성과 보안을 강화합니다.
  - **버퍼 오버플로우 방지:** 스택과 힙을 분리함으로써, 버퍼 오버플로우로 인한 보안 취약점을 일정 부분 완화할 수 있습니다.
    **3. 성능 최적화**
  - **캐시 사용 최적화:** 스택은 지역 변수에 대해 높은 시간적 국소성을 가지고 있어 캐시 효율성이 높습니다. 힙은 동적 할당에 적합하게 설계되어 있습니다.
  - **데이터 접근 최적화:** 프로그램의 다양한 데이터 접근 패턴을 효율적으로 처리하기 위해 메모리 영역을 분리합니다.
    **4. 프로그래밍의 유연성**
  - **동적 할당 지원:** 힙 영역은 프로그래머에게 동적 메모리 할당의 유연성을 제공합니다. 이는 런타임에 데이터 구조의 크기가 결정되는 경우에 필수적입니다.
  - **스택의 자동 메모리 관리:** 스택은 함수의 지역 변수 및 매개변수에 대해 간단하고 효율적인 메모리 사용을 가능하게 합니다.
- **스레드의 주소공간은 어떻게 구성되어 있을까요?**

  스레드(Thread)는 프로세스 내에서 실행되는 실행 단위로, 주로 프로세스의 자원과 주소 공간을 공유합니다. 그러나 스레드 각각은 독립적인 실행 흐름과 일부 별도의 실행 컨텍스트를 가집니다.

  - **스레드의 주소 공간 구성**
    - **코드 세그먼트:** 스레드는 프로세스의 코드 세그먼트를 공유합니다. 이는 실행 가능한 프로그램 코드를 포함하며, 모든 스레드가 동일한 함수, 명령어에 접근할 수 있습니다.
    - **데이터 세그먼트:** 전역 변수와 정적 변수가 저장되는 데이터 세그먼트도 스레드 간에 공유됩니다. 이는 프로세스 수준에서 유지되며, 모든 스레드에 의해 접근될 수 있습니다.
    - **힙 세그먼트:** 동적으로 할당된 메모리(예: C언어의 **`malloc`**, 파이썬의 객체)는 힙 세그먼트에 저장되며, 이 역시 프로세스 내의 모든 스레드에 의해 공유됩니다.
    - **스택 세그먼트:** 각 스레드는 고유한 스택을 가집니다. 스레드의 스택에는 해당 스레드의 실행 컨텍스트(지역 변수, 매개변수, 반환 주소 등)가 저장됩니다.
  - **스레드의 독립적인 부분**
    - **스레드 스택:** 각 스레드는 자신만의 스택을 가지며, 여기에는 함수 호출 시의 지역 변수, 매개변수, 반환 정보 등이 저장됩니다.
    - **스레드 로컬 스토리지(Thread Local Storage, TLS):** 스레드별로 별도로 할당되는 데이터 영역으로, 각 스레드가 서로 다른 데이터를 가질 수 있도록 합니다.
    - **스레드 ID:** 각 스레드는 고유한 식별자를 가지며, 이를 통해 다른 스레드와 구분됩니다.
  - **공유되는 부분과 독립적인 부분의 중요성**
    - **자원 공유:** 스레드 간의 코드, 데이터, 힙 세그먼트 공유는 메모리 사용 효율성을 높이고, 프로세스 내의 스레드 간 통신을 용이하게 합니다.
    - **독립 실행:** 각 스레드의 독립적인 스택과 TLS는 동시 실행 중인 여러 스레드가 서로 간섭 없이 독립적으로 작동할 수 있도록 합니다.

- **"스택"영역과 "힙"영역은 정말 자료구조의 스택/힙과 연관이 있는 걸까요? 만약 그렇다면, 각 주소공간의 동작과정과 연계해서 설명해 주세요.**
  - **프로세스의 스택 영역**
    프로세스의 스택 영역은 함수 호출과 관련된 정보(지역 변수, 매개변수, 반환 주소 등)를 저장하는 데 사용되는 LIFO (Last In, First Out) 구조의 메모리 영역입니다.
  - 자료구조의 스택과의 관련성:
    - **LIFO 구조:** 자료구조의 스택처럼, 프로세스의 스택 영역도 LIFO 방식으로 작동합니다. 가장 최근에 들어간 데이터(가장 마지막에 호출된 함수의 정보)가 가장 먼저 나옵니다.
    - **함수 호출:** 함수 호출 시 스택에 데이터가 "푸시(push)"되고, 함수가 반환되면 데이터가 "팝(pop)"됩니다. 이는 자료구조의 스택에서 데이터를 추가하고 제거하는 방식과 유사합니다.
  - **프로세스의 힙 영역**
    프로세스의 힙 영역은 동적 메모리 할당을 위한 메모리 영역입니다. 프로그램 실행 중에 필요한 메모리 공간을 런타임에 할당하고 해제합니다.
  - 자료구조의 힙과의 관련성:
    - **동적 메모리 할당:** 프로세스의 힙 영역은 동적으로 할당되고 해제됩니다. 이는 자료구조의 힙(우선순위 큐를 위한 힙 구조 등)과는 직접적인 관련이 없습니다.
    - **메모리 관리:** 프로세스의 힙 영역은 메모리 할당의 순서나 위치에 제한이 없어 자유롭게 확장하거나 축소될 수 있습니다. 이는 자료구조의 힙과는 다른 특성입니다.
  - **결론**
    프로세스의 스택 영역은 자료구조의 스택과 유사한 LIFO 방식의 작동 원리를 가지고 있으며, 함수 호출의 컨텍스트 관리에 사용됩니다. 반면, 프로세스의 힙 영역은 동적 메모리 할당에 사용되며, 자료구조의 힙과는 다른 독립적인 개념입니다. 따라서, 두 영역 모두 프로세스의 메모리 관리와 관련하여 중요한 역할을 하지만, 자료구조와는 직접적인 관련이 없거나 매우 제한적입니다.
- **그렇다면 동적 메모리 할당이란 무엇인가요? 여기서 메모리 누수는 어떤 상황일 때 발생할까요?**
  - 동적 메모리 할당(Dynamic Memory Allocation)은 프로그램 실행 중에 필요한 메모리를 런타임에 할당하는 프로세스입니다. 이는 프로그램이 실행될 때 메모리의 크기가 결정되지 않고, 실행 중에 필요에 따라 메모리를 할당하거나 해제할 수 있게 합니다.
  - 메모리 누수(Memory Leak)는 프로그램에서 동적으로 할당된 메모리 영역이 더 이상 필요 없음에도 불구하고 해제되지 않아, 시스템의 메모리가 점점 고갈되는 현상을 말합니다.
- **IPC의 Shared Memory 기법은 프로세스 주소공간의 어디에 들어가나요? 그런 이유가 있을까요?**
  - IPC(Inter-Process Communication)의 Shared Memory 기법은 다수의 프로세스들이 메모리의 동일한 부분을 공유하여 데이터를 교환하는 방식입니다. 이 공유 메모리 영역은 일반적으로 프로세스 주소 공간 내의 힙 영역에 위치합니다.
  1. **데이터 공유 용이성:** Shared Memory는 프로세스들이 메모리의 같은 부분을 직접 읽고 쓸 수 있게 하여, 효율적인 데이터 공유를 가능하게 합니다.
  2. **데이터 공유 용이성:** Shared Memory는 프로세스들이 메모리의 같은 부분을 직접 읽고 쓸 수 있게 하여, 효율적인 데이터 공유를 가능하게 합니다.
  3. **성능 향상:** Shared Memory는 다른 IPC 방법(예: 파이프, 메시지 큐)에 비해 데이터를 교환하는 데 있어 오버헤드가 적고, 더 빠른 성능을 제공합니다.
  4. **동적 할당의 유연성:** 힙 영역에서의 동적 메모리 할당 덕분에, Shared Memory의 크기를 필요에 따라 조정할 수 있습니다.
- **스택과 힙영역의 크기는 언제 결정되나요? 프로그램 개발자가 아닌, 사용자가 이 공간의 크기를 수정할 수 있나요?**
  - **스택 영역의 크기 결정**
    1. **초기 크기:** 스택의 초기 크기는 대부분 컴파일 시에 결정됩니다. 컴파일러 설정에 따라 기본 스택 크기가 정해집니다.
    2. **최대 크기:** 스택의 최대 크기 역시 운영체제나 컴파일러 옵션에 의해 결정될 수 있습니다. 일부 시스템에서는 개발자가 컴파일 시에 스택 크기를 조정할 수 있는 옵션을 제공합니다.
  - **힙 영역의 크기 결정**
    1. **동적 할당:** 힙 영역은 프로그램 실행 중에 동적으로 할당되고 관리됩니다. 따라서 초기 크기는 작지만, 필요에 따라 런타임에 증가할 수 있습니다.
    2. **시스템 리소스에 따라 달라짐:** 힙 영역의 최대 크기는 시스템의 가용 메모리와 운영체제의 정책에 따라 다릅니다.
  - **사용자에 의한 크기 수정 가능성**
    - **스택 크기:** 일반적인 사용자는 스택 크기를 직접 변경할 수 없습니다. 이는 주로 개발 과정에서 컴파일러 설정을 통해 조정됩니다. 그러나 일부 운영체제에서 고급 사용자나 관리자가 특정 프로그램의 스택 크기를 조정할 수 있는 옵션을 제공할 수도 있습니다.
    - **힙 크기:** 사용자는 힙 크기를 직접 설정하지 않습니다. 힙 영역은 프로그램의 동적 메모리 요청에 따라 자동으로 관리되며, 프로그램의 메모리 사용 패턴과 시스템의 메모리 관리 정책에 의해 크기가 결정됩니다.
- **재귀함수의 작동 원리를 call stack 과 관련해서 설명해주세요.**
  - 재귀 함수는 기본적으로 자기 자신을 호출하는 함수입니다. 이러한 재귀적 호출은 각각의 호출이 콜 스택에 기록되어 함수의 실행 상태를 관리하는 방식으로 진행됩니다.
  - **재귀 함수의 호출 과정**
    1. **처음 호출:** 재귀 함수가 처음 호출되면, 그 함수의 매개변수, 지역 변수, 반환 주소 등이 콜 스택에 푸시됩니다. 이는 일반 함수 호출과 동일합니다.
    2. **자기 자신 호출:** 재귀 함수 내에서 함수는 자기 자신을 다시 호출합니다. 이 새로운 호출에 대한 정보 역시 콜 스택에 푸시됩니다. 각 재귀 호출은 독립적인 스택 프레임을 가지며, 이전 호출과는 구분됩니다.
    3. **베이스 케이스 도달:** 재귀 함수는 베이스 케이스(종료 조건)에 도달할 때까지 계속해서 자기 자신을 호출합니다. 베이스 케이스에 도달하면, 더 이상 새로운 재귀 호출을 하지 않고 값을 반환합니다.
    4. **스택 언와인딩:** 베이스 케이스에 도달하면, 재귀 함수의 각 인스턴스(각각의 호출)는 값을 반환하고 콜 스택에서 제거됩니다. 이 과정은 마지막 재귀 호출부터 시작하여 처음 호출한 함수로 거슬러 올라갑니다.

### **5. 단기, 중기, 장기 스케쥴러에 대해 설명해 주세요.**

- **현대 OS에는 단기, 중기, 장기 스케쥴러를 모두 사용하고 있나요?**
- **프로세스의 스케쥴링 상태에 대해 설명해 주세요.**
- **preemptive/non-preemptive 에서 존재할 수 없는 상태가 있을까요?**
- **Memory가 부족할 경우, Process는 어떠한 상태로 변화할까요?**

### **6. 컨텍스트 스위칭 시에는 어떤 일들이 일어나나요?**

- **프로세스와 스레드는 컨텍스트 스위칭이 발생했을 때 어떤 차이가 있을까요?**
- **컨텍스트 스위칭이 발생할 때, 기존의 프로세스 정보는 커널스택에 어떠한 형식으로 저장되나요?**
- **컨텍스트 스위칭은 언제 일어날까요?**

### **7. 프로세스 스케줄링 알고리즘에는 어떤 것들이 있나요?✅**

- **프로세스 스케줄링 알고리즘**
  - **First-Come, First-Served (FCFS):** 가장 간단한 스케줄링 알고리즘으로, 먼저 도착한 프로세스를 먼저 처리합니다.
    - A (17ms) → B(5ms) → C(2ms) 순서대로 처리되면, C 는 22ms 나 기다려야 한다. 이것을 convoy effect 라고 한다.
  - **Shortest Job First (SJF):** CPU 버스트 길이가 가장 짧은 프로세스를 먼저 스케줄링합니다. 예측이나 과거의 행동에 기반할 수 있습니다.
  - **Round Robin (RR):** 각 프로세스에 동일한 크기의 시간 할당량(time slice)을 주고, 순환 방식으로 프로세스를 스케줄링합니다.
  - **SRT(Shortest Remaining Time)** : SJF + RR 을 합친 형태이다. 프로세스들은 정해진 타임 슬라이스만큼 CPU를 사용하되, CPU를 사용할 다음 프로세스는 작업 시간이 가장 적은 프로세스가 선택된다.
  - **Priority Scheduling:** 각 프로세스에 우선순위를 부여하고, 가장 높은 우선순위를 가진 프로세스를 먼저 스케줄링합니다.
    - 우선순위가 낮은 프로세스는 영영 실행 안되는 starvation 문제 발생할 수 도 있다.
    - 이를 위해 오랫동안 대기한 프로세스의 우선순위를 점차 높이는 aging 기법이 있다.
  - **Multilevel Queue Scheduling:** 프로세스를 여러 큐에 분류하고, 각 큐에 다른 스케줄링 알고리즘을 적용합니다.
- **RR을 사용할 때, Time Slice에 따른 trade-off를 설명해 주세요.**
  - **짧은 Time Slice:** 응답 시간은 개선되지만, 컨텍스트 스위칭이 빈번하게 발생하여 오버헤드가 증가합니다.
  - **긴 Time Slice:** 컨텍스트 스위칭 오버헤드는 감소하지만, 응답 시간이 증가할 수 있으며, RR 스케줄링이 FCFS와 유사해질 수 있습니다.
- **싱글 스레드 CPU 에서 상시로 돌아가야 하는 프로세스가 있다면, 어떤 스케쥴링 알고리즘을 사용하는 것이 좋을까요? 또 왜 그럴까요?**
  - 정해진 답은 없다. 각 알고리즘마다 장단점이 있다.
  - RR → 모든 프로세스에게 CPU 사용 시간을 동등하게 제공
- **동시성과 병렬성의 차이에 대해 설명해 주세요.**
- **타 스케쥴러와 비교하여, Multi-level Feedback Queue는 어떤 문제점들을 해결한다고 볼 수 있을까요?**
  - Multilevel Queue Scheduling 에서는 프로세스들이 큐 사이를 이동할 수 없다. 이런 방식은, 우선순위가 낮은 프로세스는 계속 연기될 여지가 있다. → starvation 현상 발생
  - **Multi-level Feedback Queue** 에서는 프로세스들이 큐 사이를 이동할 수 있다. 새로 준비 상태가 된 프로세스가 있다면 우선순위가 가장 높은 우선순위 큐에 삽입되고, 일정 시간(타임슬라이스) 만큼 실행된다. 만약 프로세스가 해당 큐에서 실행이 끝나지 않는다면 다음 큐에 삽입되어 실행되고, 또 끝나지 않으면 우선순위가 계속 내려간다.
  - 자연적으로 CPU 를 비교적 오래 사용하는 CPU bound process 는 우선순위가 낮아지고, CPU를 비교적 적게 사용하는 I/O bound process 는 우선순위가 높은 큐에서 실행이 끝난다.
- **FIFO 스케쥴러는 정말 쓸모가 없는 친구일까요? 어떤 시나리오에 사용하면 좋을까요?**
  - **사용 시나리오**
    1. 단순성 및 예측 가능성이 중요한 경우
    - **단순한 시스템:** FIFO는 구현이 매우 간단하므로, 복잡도를 최소화하려는 작은 규모 또는 임베디드 시스템에서 유용합니다.
    - **작업 순서 보장:** 특정 작업들이 도착한 순서대로 처리되어야 하는 시나리오에서 FIFO는 이러한 순서를 보장합니다.
    1. 짧은 작업이 많은 경우
    - **작업 시간의 예측:** 모든 작업의 실행 시간이 짧고 비슷하다면, FIFO는 간단하고 효율적인 방법이 될 수 있습니다.
    1. 부하가 낮은 시스템
    - **낮은 CPU 사용률:** 시스템에 동시에 실행되는 프로세스의 수가 적고 CPU 사용률이 낮을 때, FIFO는 간단하고 효율적으로 작업을 처리할 수 있습니다.
- **우리는 스케줄링 알고리즘을 "프로세스" 스케줄링 알고리즘이라고 부릅니다. 스레드는 다른 방식으로 스케줄링을 하나요?**
  - 스케줄링 알고리즘을 "프로세스" 스케줄링 알고리즘이라고 부르는 것은 전통적으로 프로세스가 운영 체제에서 실행 단위로 사용되었기 때문입니다. 그러나 현대의 운영 체제에서는 "스레드"가 실제 스케줄링의 기본 단위로 더 자주 사용됩니다. 스레드는 프로세스 내에서 동작하는 더 작은 실행 단위이며, 각각 독립적인 실행 흐름을 가집니다.
- **유저 스레드와 커널 스레드의 스케쥴링 알고리즘은 똑같을까요?**
  - **유저 스레드 (User Threads)**
    1. **정의:** 유저 스레드는 사용자 수준의 라이브러리에서 구현되며, 운영 체제 커널은 이 스레드의 존재를 인식하지 못합니다.
    2. **스케줄링:** 유저 스레드는 운영 체제에 의해 직접 스케줄링되지 않습니다. 대신 사용자 수준의 스레드 라이브러리(예: POSIX Pthreads)에 의해 스케줄링되며, 이러한 라이브러리는 일반적으로 자체 스케줄링 알고리즘을 사용합니다.
    3. **장점:** 유연성이 뛰어나며, 커널이 지원하지 않는 스레딩 모델을 구현할 수 있습니다.
    4. **단점:** 커널이 스레드의 존재를 모르기 때문에, 하나의 유저 스레드가 차단되면 같은 프로세스 내의 다른 스레드도 영향을 받을 수 있습니다.
  - **커널 스레드 (Kernel Threads)**
    1. **정의:** 커널 스레드는 운영 체제의 커널에서 직접 관리됩니다. 커널은 각 스레드를 개별적인 실행 단위로 인식하고 관리합니다.
    2. **스케줄링:** 커널 스레드는 운영 체제의 스케줄러에 의해 직접 스케줄링됩니다. 이 때 일반적인 프로세스 스케줄링 알고리즘(예: Round-Robin, Priority Scheduling 등)이 적용됩니다.
    3. **장점:** 멀티코어 프로세서에서의 병렬 처리에 최적화되어 있으며, 하나의 스레드가 차단되어도 다른 스레드에 영향을 미치지 않습니다.
    4. **단점:** 유저 스레드보다 컨텍스트 스위치 비용이 높을 수 있습니다.

### **8. 뮤텍스와 세마포어의 차이점은 무엇인가요?✅**

- **뮤텍스 vs. 세마포어**
  - 뮤텍스 (Mutex)
    - **목적:** 뮤텍스는 Mutual Exclusion(상호 배제)을 위해 사용됩니다. 한 번에 하나의 스레드만이 뮤텍스를 소유할 수 있으며, 다른 스레드는 그 뮤텍스가 방출될 때까지 접근할 수 없습니다.
    - 뮤텍스는 Locking 메커니즘으로 오직 하나의 쓰레드만이 동일한 시점에 뮤텍스를 얻어 임계 영역(Critical Section)에 들어올 수 있다. 그리고 오직 이 쓰레드만이 임계 영역에서 나갈 때 뮤텍스를 해제할 수 있다.
      ```bash
      acquire();
      // 임계 구역
      release();
      ```
    - **용도:** 주로 공유 자원에 대한 접근을 동기화하는데 사용됩니다.
    - **소유권:** 뮤텍스는 소유 개념이 있습니다. 즉, 락을 얻은 스레드만이 락을 방출할 수 있습니다.
  - 세마포어 (Semaphore)
    - Signaling 메카니즘이다. 세마포어는 락을 걸지 않은 스레드도 signal 을 보내 락을 해제할 수 있다.
    - **목적:** 세마포어는 카운팅 세마포어와 이진 세마포어 두 가지 형태가 있습니다. 일반적으로 리소스의 개수를 제한하는데 사용됩니다.
      ```bash
      wait(); // 임계구역에 들어가도 좋은지, 기다려야 하는지 알려줌 -> S(공유가능리소스) 1개 감소
      // 임계구역
      signal(); // 임계 구역 앞에서 기다리는 프로세스에게 '가도좋다'를 알려줌 -> S++
      ```
    - **용도:** 여러 개의 리소스에 대한 동시 접근을 제어하는데 적합합니다.
    - **소유권:** 세마포어는 소유 개념이 없습니다. 하나의 스레드가 세마포어를 감소시키면, 다른 스레드가 그 세마포어를 증가시킬 수 있습니다.
  - 차이
    - **뮤텍스**는 Locking 메커니즘으로 락을 걸은 쓰레드만이 임계 영역을 나갈때 락을 해제할 수 있다. 하지만 **세마포어**는 Signaling 메커니즘으로 락을 걸지 않은 쓰레드도 signal을 사용해 락을 해제할 수 있다. 세마포어의 카운트를 1로 설정하면 뮤텍스처럼 활용할 수 있다.
- **이진 세마포어와 뮤텍스의 차이에 대해 설명해 주세요.**

  - Binary Semaphore vs. Mutex

    ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/4b2b6f1d-958d-4919-8f52-6c8b77133337/426e23a4-b8da-4b25-9580-c8feca709cac/Untitled.png)

  -

- **Lock을 얻기 위해 대기하는 프로세스들은 Spin Lock 기법을 사용할 수 있습니다. 이 방법의 장단점은 무엇인가요? 단점을 해결할 방법은 없을까요?**

  - Spin Lock은 다중 스레드 또는 프로세스 환경에서 동기화를 위해 사용되는 기법으로, 프로세스나 스레드가 락(Lock)을 획득하기 위해 활성 대기(Active Waiting) 또는 "스핀(Spin)"하는 방식입니다.
  - 스핀 락(Spin lock)은 임계 구역에 진입이 불가능할 때 진입이 가능할 때까지 루프를 돌면서 재시도하는 방식으로 구현된 락을 가리킵니다.

    ```bash
    wait(S) {
      while (S <= 0);  // 자원이 없다면 while 루프를 돌며 대기를 함.

      S--;  // 자원을 획득함.
    }

    signal(S) {
      S++;  // 자원을 해제함.
    }
    ```

  - **장점**
    1. **오버헤드 감소**: 컨텍스트 스위치가 발생하지 않기 때문에, 락을 기다리는 동안 발생할 수 있는 오버헤드가 감소합니다. 컨텍스트 스위치는 자원을 많이 소모하는 작업이므로, 이를 피하는 것은 시스템의 전반적인 성능을 향상시킬 수 있습니다.
    2. **응답 시간 개선**: 락이 곧바로 사용 가능해질 것으로 예상될 때, Spin Lock은 다른 락 기법에 비해 더 빠른 응답 시간을 제공할 수 있습니다. 락이 잠시만 필요한 경우에 특히 유용합니다.
    3. **구현의 단순성**: Spin Lock은 구현하기 상대적으로 간단합니다. 복잡한 락 관리 로직이 필요 없어 시스템의 단순성을 유지할 수 있습니다.
  - **단점**
    1. **CPU 자원의 낭비**: 스레드가 락을 얻기 위해 계속해서 CPU 시간을 소비하면서 대기하는 것은 자원의 낭비를 초래할 수 있습니다. 특히 락이 오랜 시간 동안 사용 불가능한 경우에 이러한 낭비가 심해질 수 있습니다.
    2. **스레드 우선순위 역전**: 우선순위가 낮은 스레드가 락을 소유하고 있고 우선순위가 높은 스레드가 해당 락을 기다리는 경우, 우선순위 역전 문제가 발생할 수 있습니다.
    3. **데드락의 가능성**: 잘못 설계된 Spin Lock은 데드락에 빠질 위험이 있습니다. 예를 들어, 두 스레드가 서로 다른 순서로 락을 획득하려고 할 때 데드락이 발생할 수 있습니다.
  - **단점 해결 방법**
    1. **Adaptive Spin Locks**: Spin Lock 기법과 블록킹 기법을 혼합하여 사용하는 것입니다. 예를 들어, 스레드가 일정 시간 동안만 스핀하고, 그 후에는 블록킹 모드로 전환하여 CPU 자원을 절약할 수 있습니다.
    2. **우선순위 상속**: 우선순위 역전 문제를 해결하기 위해, 락을 보유한 스레드가 더 높은 우선순위의 스레드의 우선순위를 상속받도록 하는 방법입니다. 이를 통해 우선순위가 높은 스레드가 더 빨리 실행될 수 있도록 합니다.
    3. **Backoff 알고리즘**: 스레드가 락을 얻기 위해 반복적으로 시도하는 간격을 점진적으로 늘리는 방식입니다. 이는 락 경합이 심한 상황에서 효과적일 수 있으며, CPU 자원의 낭비를 줄일 수 있습니다.
    4. **데드락 감지 및 해결**: 시스템이 데드락을 감지하고 해결할 수 있는 메커니즘을 구현함으로써, Spin Lock의 데드락 문제를 완화할 수 있습니다.

- **뮤텍스와 세마포어 모두 커널이 관리하기 때문에, Lock을 얻고 방출하는 과정에서 시스템 콜을 호출해야 합니다. 이 방법의 장단점이 있을까요? 단점을 해결할 수 있는 방법은 없을까요?**
  - **장점**
    1. **동기화의 안정성**: 커널 수준에서 관리되기 때문에 뮤텍스와 세마포어는 높은 수준의 동기화 안정성을 제공합니다. 커널은 다양한 프로세스와 스레드 간의 동기화를 효과적으로 관리할 수 있습니다.
    2. **프로세스 간 통신**: 뮤텍스와 세마포어는 프로세스 간 통신(IPC)에서도 사용될 수 있습니다. 이들은 서로 다른 프로세스 간의 자원 공유와 동기화를 가능하게 합니다.
    3. **데드락 예방과 감지**: 커널은 데드락을 예방하고 감지하는 메커니즘을 제공할 수 있습니다. 이를 통해 데드락과 관련된 문제를 해결하고 시스템의 안정성을 높일 수 있습니다.
  - **단점**
    1. **오버헤드**: 시스템 콜은 사용자 모드와 커널 모드 간의 컨텍스트 스위치를 필요로 합니다. 이러한 스위치는 상당한 오버헤드를 발생시키며, 특히 고성능이 요구되는 환경에서는 이 오버헤드가 문제가 될 수 있습니다.
    2. **성능 저하**: 시스템 콜의 사용은 성능 저하를 야기할 수 있습니다. 락을 자주 요청하고 해제하는 애플리케이션의 경우, 이러한 오버헤드가 누적되어 전체적인 성능에 영향을 미칠 수 있습니다.
    3. **확장성 제한**: 커널 기반 동기화 메커니즘은 확장성에 제한이 있을 수 있습니다. 많은 수의 스레드나 프로세스가 동시에 락을 요청할 때, 성능 저하가 발생할 수 있습니다.
  - **단점 해결 방법**
    1. **경량화된 락**: 사용자 수준의 경량화된 락 구현(예: 스핀락)을 사용하여 커널 모드로의 전환을 피할 수 있습니다. 이는 특히 락 보유 시간이 짧은 경우에 유용할 수 있습니다.
    2. **커널 경량화**: 최신 운영체제는 커널 경량화 기법을 사용하여 시스템 콜의 오버헤드를 최소화합니다. 예를 들어, futex (Fast Userspace Mutex)와 같은 메커니즘은 사용자 공간에서 락을 관리하고 필요한 경우에만 커널에 개입합니다.
    3. **비차단 알고리즘**: 락-프리(Lock-Free) 또는 웨이트-프리(Wait-Free) 알고리즘과 같은 비차단 동기화 알고리즘을 사용하여 오버헤드를 줄일 수 있습니다.
    4. **락의 세분화**: 락의 범위를 세분화하여 한 번에 하나의 스레드만이 아니라 여러 스레드가 동시에 작업을 수행할 수 있도록 함으로써 병렬 처리를 향상시킬 수 있습니다.

### **9. Deadlock 에 대해 설명해 주세요. ✅**

- **Deadlock 이 동작하기 위한 4가지 조건에 대해 설명해 주세요.**
  - 상호 배제(mutual exclusion) : 한 프로세스가 사용하는 자원을 다른 프로세스가 사용할 수 없을 때
  - 점유와 대기(hold and wait) : 자원을 할당받은 상태에서 다른 자원을 할당받기를 기다리는 상태
  - 비선점(nonpreemptive) : 프로세스가 자원을 얻은 후에는 그 자원을 스스로 해제할 때까지 다른 프로세스에 의해 강제로 빼앗길 수 없습니다.
  - 원형 대기(circular wait) : 두 개 이상의 프로세스가 순환 형태로 자원을 기다리는 상태입니다. 예를 들어, 프로세스 A가 자원 X를 가지고 프로세스 B가 필요로 하는 자원 Y를 기다리고, 프로세스 B는 자원 Y를 가지고 프로세스 A가 필요로 하는 자원 X를 기다리는 상황입니다.
- **그렇다면 3가지만 충족하면 왜 Deadlock 이 발생하지 않을까요?**
  - 이 네 가지 조건 중 하나라도 충족되지 않으면 데드락이 발생하지 않습니다. 예를 들어, 비선점 조건이 충족되지 않으면, 시스템이 필요할 때 어떤 프로세스로부터 자원을 강제로 회수할 수 있으므로 데드락 상태가 해결될 수 있습니다. 또한 순환 대기 조건이 충족되지 않으면, 자원을 기다리는 프로세스 간에 순환적인 대기 사슬이 형성되지 않아 데드락이 발생하지 않습니다. 상호 배제나 보유 대기 조건이 충족되지 않아도 마찬가지입니다.
- **어떤 방식으로 예방할 수 있을까요?**
  1. **상호 배제 조건 극복**: 이 조건을 완전히 제거하는 것은 불가능할 수 있으나, 자원을 공유 가능하게 만들어 상호 배제의 필요성을 최소화할 수 있습니다. 예를 들어, 일부 자원(프린터 등)을 여러 프로세스가 공유할 수 있도록 설계함으로써 상호 배제를 줄일 수 있습니다.
  2. **보유 대기 조건 극복**: 프로세스가 실행되기 전에 필요한 모든 자원을 한 번에 요청하고 할당받도록 하여 이 조건을 방지할 수 있습니다. 즉, 운영체제가 특정 프로세스에 자원을 모두 할당하거나, 아예 할당하지 않는 식으로 배분하는 것이다. 이 방법은 자원 활용도가 낮아지는 단점이 있지만, 데드락을 효과적으로 예방할 수 있습니다.
  3. **비선점 조건 극복**: 이미 할당된 자원을 다른 프로세스가 요구할 경우, 이를 선점할 수 있도록 하는 것입니다. 프로세스가 자원을 기다리는 동안 현재 보유한 자원을 해제하고 필요 시 다시 요청하도록 함으로써 데드락을 예방할 수 있습니다.
  4. **순환 대기 조건 극복**: 모든 자원에 고유한 번호를 할당하고, 프로세스가 번호 순서대로 자원을 요청하도록 함으로써 이 조건을 방지할 수 있습니다. 이 방법은 프로세스가 높은 번호의 자원을 보유하고 있을 때 낮은 번호의 자원을 요청하지 못하도록 하여 순환 대기를 예방합니다.
- **왜 현대 OS는 Deadlock을 처리하지 않을까요?**
  현대의 운영체제가 데드락을 처리하지 않는 주된 이유는 데드락 예방, 회피 및 감지 알고리즘의 복잡성과 자원 관리에 따른 성능상의 비효율 때문입니다. 데드락 처리에 관련된 몇 가지 중요한 고려사항은 다음과 같습니다:
  1. **성능 저하**: 데드락 예방 및 회피 알고리즘은 종종 시스템의 성능을 저하시킬 수 있습니다. 예를 들어, 데드락 예방을 위해 자원을 보수적으로 할당하면 자원 활용도가 낮아지고 시스템의 처리량이 감소할 수 있습니다.
  2. **복잡성**: 데드락 감지 및 복구 알고리즘은 복잡하며, 실시간으로 데드락을 정확히 감지하고 해결하는 것은 어려울 수 있습니다. 이는 시스템 설계와 운영을 복잡하게 만들며, 오류의 가능성을 증가시킵니다.
  3. **자원 관리**: 운영체제가 모든 자원에 대해 데드락을 관리하고 감지하는 것은 매우 부담스러운 작업입니다. 특히 대규모 시스템에서는 다양한 자원과 프로세스 간의 상호 작용이 매우 복잡해질 수 있습니다.
  4. **비용 대비 효과**: 데드락은 일반적으로 잘 설계된 시스템에서는 드물게 발생합니다. 따라서 데드락 처리를 위해 상당한 시스템 리소스를 할당하는 것은 비용 대비 효과가 낮을 수 있습니다.
  5. **응용 프로그램 수준의 해결**: 많은 경우, 데드락 문제는 응용 프로그램 수준에서 더 효과적으로 해결될 수 있습니다. 응용 프로그램 개발자는 자신의 프로그램이 사용하는 특정 자원과 요구 사항을 잘 알고 있으므로, 데드락을 방지하거나 해결하기 위한 전략을 적용하기가 더 쉽습니다.
  6. **실용적 접근**: 현대 운영체제는 데드락 발생 가능성을 최소화하기 위한 설계 전략을 채택하는 경향이 있습니다. 예를 들어, 자원 할당 전략을 최적화하거나, 시스템의 전반적인 설계를 통해 데드락의 위험을 줄이는 방향으로 작업합니다.
     따라서 현대의 운영체제는 일반적으로 데드락을 완전히 예방하거나 처리하는 데 집중하기보다는, 시스템 설계와 프로그래밍 관행을 통해 데드락의 가능성을 줄이는 데 집중합니다.
- **Wait Free와 Lock Free를 비교해 주세요.**

### **10. 프로그램이 컴파일 되어, 실행되는 과정을 간략하게 설명해 주세요. ✅**

프로그램은 디스크에 이진 실행 파일로 존재한다. CPU에서 실행하려면 이 프로그램을 메모리로 가져와서 프로세스 형태로 배치해야 한다.

1. **소스 코드 작성**: 개발자가 특정 프로그래밍 언어로 소스 코드를 작성합니다.
2. **컴파일**: 컴파일러는 소스 코드를 기계어 코드(또는 바이트 코드) (=오브젝트 파일) 로 변환합니다. 이 과정에서 문법 검사, 타입 검사 등이 수행됩니다. 오브젝트 파일은 임의 의 물리 메모리 위치에 적재되도록 설계된 파일이다.
3. **링킹**: 컴파일된 코드(오브젝트 코드)에 필요한 라이브러리나 다른 오브젝트 파일들을 연결하여 실행 가능한 파일을 생성합니다. 이 과정을 **링커**가 담당합니다.
4. **로딩**: 생성된 실행 파일을 메모리에 적재합니다. 이때 **로더**가 실행 파일을 메모리에 적재하고 실행을 준비합니다.
5. **실행**: 프로세스가 메모리에 적재되면 CPU는 명령어를 실행하기 시작합니다.

- **링커와, 로더의 차이에 대해 설명해 주세요.**
  - **링커(Linker)**: 컴파일러가 생성한 하나 이상의 오브젝트 파일들을 결합하고, 필요한 라이브러리 함수들을 추가하여 실행 가능한 파일을 만드는 역할을 합니다.
  - **로더(Loader)**: 디스크에 저장된 실행 파일을 메모리에 적재하여 실행을 위해 준비하는 역할을 합니다. 로더는 실행 파일의 코드와 데이터를 메모리에 적절한 위치에 복사하고, 실행을 시작합니다.
- **컴파일 언어와 인터프리터 언어의 차이에 대해 설명해 주세요.**
  - **컴파일 언어**: 소스 코드 전체를 먼저 기계어로 번역한 후 실행 파일을 생성하여 실행합니다. C, C++ 등이 여기에 속합니다. 컴파일 과정을 거치기 때문에 실행 속도가 빠릅니다.
  - **인터프리터 언어**: 소스 코드를 한 줄씩 읽어서 바로 실행하는 방식입니다. Python, JavaScript 등이 여기에 속합니다. 별도의 컴파일 과정 없이 바로 실행할 수 있지만, 컴파일 언어에 비해 실행 속도가 느릴 수 있습니다.
- **JIT(Just-In-Time) Compiler 에 대해 설명해 주세요.**
  - IT 컴파일러는 프로그램의 실행 시간 중에 필요할 때 실시간으로 바이트 코드를 기계어 코드로 변환하는 기술입니다. 이 방식은 인터프리터 언어의 실행 속도를 향상시키기 위해 사용됩니다. 예를 들어, Java의 JVM(Java Virtual Machine)이나 .NET의 CLR(Common Language Runtime)이 JIT 컴파일을 사용합니다.
- **본인이 사용하는 언어는, 어떤식으로 컴파일 및 실행되는지 설명해 주세요.**
- **Python 같은 언어는 CPython, Jython, PyPy등의 다양한 구현체가 있습니다. 각각은 어떤 차이가 있을까요? 또한, 실행되는 과정 또한 다를까요?**
  - **CPython**: Python의 기본 구현체로, Python 코드를 바이트 코드로 컴파일한 다음 인터프리트하여 실행합니다.
  - **Jython**: Java 플랫폼 위에서 실행되며, Python 코드를 자바 바이트 코드로 변환하여 JVM에서 실행합니다.
  - **PyPy**: JIT 컴파일 기술을 사용하여 Python 코드의 실행 속도를 향상시킨 구현체입니다. 실행 과정 중에 자주 사용되는 코드를 식별하고, 해당 코드를 기계어로 컴파일하여 실행 속도를 빠르게 합니다.
- **우리는 흔히 fork(), exec() 시스템 콜을 사용하여 프로세스를 적재할 수 있다고 배웠습니다. 로더의 역할은 이 시스템 콜과 상관있는 걸까요? 아니면 다른 방식으로 프로세스를 적재할 수 있는 건가요?**
  - **`fork()`**와 **`exec()`** 시스템 콜은 프로세스를 생성하고 새 프로그램을 실행하는 데 사용됩니다. **`fork()`**는 호출한 프로세스의 복사본을 생성하고, **`exec()`**는 새 프로그램을 메모리에 적재하여 실행합니다. 이 과정에서 로더가 **`exec()`**에 의해 실행 파일을 메모리에 적재하는 역할을 합니다. 따라서, 로더는 이 시스템 콜과 직접적인 관련이 있으며, 프로세스를 적재하는 중요한 역할을 담당합니다.

### **11. IPC가 무엇이고, 어떤 종류가 있는지 설명해 주세요. ✅**

협력적인 프로세스들은 데이터를 교환할 수 있는, 즉 서로 데이터를 보내거나 받을 수 있는 프로세스 간 통신(IPC) 기법이 필요하다. 프로세스 간 통신에는 기본적으로 공유 메모리(shared memory)와 메세지 전달(message-passing) 두가지 모델이 있다.
메세지 전달 모델은 적은 양의 데이터를 교환하는데 유용하다.

- **Shared Memory가 무엇이며, 사용할 때 유의해야 할 점에 대해 설명해 주세요.**
  공유 메모리는 두 개 이상의 프로세스가 시스템의 동일한 물리 메모리 영역에 접근할 수 있게 하는 IPC 메커니즘입니다. 이 방법은 데이터를 복사하지 않고 직접 메모리를 공유하기 때문에, 다른 IPC 메커니즘에 비해 통신 속도가 매우 빠릅니다.
  **사용 시 유의해야 할 점**:
  - **동기화**: 여러 프로세스가 동시에 공유 메모리에 접근할 수 있으므로, 데이터 일관성과 무결성을 유지하기 위한 적절한 동기화 기법(예: 뮤텍스, 세마포어)의 사용이 필수적입니다.
  - **보안**: 공유 메모리는 프로세스 간 직접 메모리 접근을 허용하기 때문에, 잘못된 사용은 시스템의 보안 취약점으로 이어질 수 있습니다.
- **메시지 큐는 단방향이라고 할 수 있나요?**
  - 메시지 큐는 기본적으로 단방향 통신 채널입니다. 하나의 프로세스가 메시지 큐에 메시지를 보내고(쓰기 작업), 다른 프로세스가 이를 읽는(읽기 작업) 구조입니다. 그러나 실제 사용 시에는 두 프로세스가 각각 메시지 큐를 사용하여 서로에게 메시지를 보낼 수 있기 때문에, 양방향 통신을 구현하는 것도 가능합니다. 이렇게 양방향 통신을 위해 각각의 프로세스가 메시지 큐를 하나씩 사용하면, 두 개의 메시지 큐를 통해 마치 양방향 통신 채널처럼 동작시킬 수 있습니다. 따라서, 메시지 큐 자체는 단방향 통신 메커니즘이지만, 실제 응용 프로그램에서는 양방향 통신으로 활용될 수 있습니다.

### **12. Thread Safe 하다는 것은 어떤 의미인가요?✅**

멀티 쓰레딩 환경에서 여러 스레드가 동시에 같은 코드 블록이나 리소스에 접근하더라도 프로그램의 실행 결과가 올바르게 나오는 것을 의미합니다. 즉, 코드나 리소스가 여러 스레드로부터 동시에 접근되어도 예상치 못한 문제나 데이터의 불일치가 발생하지 않도록 보장하는 것입니다.

- **Thread Safe 를 보장하기 위해 어떤 방법을 사용할 수 있나요?**
  - **락(Locks):** 가장 일반적인 방법으로, 특정 코드 섹션에 동시에 하나의 스레드만 접근할 수 있도록 합니다. 뮤텍스(mutexes), 세마포어(semaphores), 모니터(monitors) 등이 여기에 해당됩니다.
  - **원자적 연산(Atomic Operations):** 원자적 연산은 중단되지 않는 연산을 의미합니다. 이 방법을 사용하면, 데이터에 대한 복잡한 동기화 없이도 값을 안전하게 변경할 수 있습니다.
  - **불변 객체(Immutable Objects):** 데이터 자체를 변경할 수 없게 만듦으로써 동기화 문제를 회피합니다. 객체가 생성될 때 그 상태가 고정되고, 그 이후에는 변경될 수 없습니다.
  - **스레드 로컬 스토리지(Thread Local Storage):** 데이터를 각 스레드의 로컬 스토리지에 저장하여, 스레드 간의 데이터 공유가 필요 없게 합니다.
- **Peterson's Algorithm 이 무엇이며, 한계점에 대해 설명해 주세요.**
  Peterson's Algorithm은 두 개의 스레드가 상호 배제(mutual exclusion)를 달성하기 위해 사용할 수 있는 알고리즘입니다. 이 알고리즘은 두 스레드가 공유 자원에 동시에 접근하지 못하도록 하여 race condition을 방지합니다. 각 스레드는 먼저 "interested" 상태를 표시하고, 다른 스레드가 실행을 완료할 때까지 기다립니다. 이 방식은 비교적 간단한 시나리오에서 효과적입니다.
  **한계점:**
  - **확장성:** Peterson's Algorithm은 두 스레드에 한정되어 있어, 더 많은 스레드가 참여하는 시스템에서는 적용하기 어렵습니다.
  - **성능:** 스레드가 자원을 기다리는 동안 바쁜 대기(busy waiting) 상태가 될 수 있으며, 이는 CPU 자원의 낭비를 초래할 수 있습니다.
  - **실용성:** 현대의 멀티코어 프로세서와 다중 프로세서 시스템에서는 메모리 일관성(memory consistency) 모델과 캐시 일관성 문제로 인해 실제 사용이 제한적일 수 있습니다.
- **Race Condition 이 무엇인가요?**
  - Race Condition은 두 개 이상의 프로세스나 스레드가 공유된 데이터에 동시에 접근하려고 할 때, 그 접근 순서에 따라 실행 결과가 달라질 수 있는 상황을 말합니다. 이러한 상황은 데이터의 일관성과 정확성을 해칠 수 있으며, 예측 불가능한 결과를 초래할 수 있습니다. Race Condition을 방지하기 위해서는 적절한 동기화 메커니즘이 필요합니다.
- **Thread Safe를 구현하기 위해 반드시 락을 사용해야 할까요? 그렇지 않다면, 어떤 다른 방법이 있을까요?**
  락을 사용하는 것은 Thread Safe를 구현하는 한 가지 방법일 뿐, 반드시 필요한 것은 아닙니다. 락 외에도 여러 방법이 있습니다:

  - **락 없는 프로그래밍(Lock-Free Programming):** 원자적 연산을 사용하여 공유 데이터에 접근합니다. 이 방법은 락을 사용할 때 발생할 수 있는 오버헤드와 교착 상태(deadlock) 문제를 피할 수 있습니다.
  - **소프트웨어 트랜잭션 메모리(Software Transactional Memory, STM):** 메모리 트랜잭션을 사용하여 코드 블록을 원자적으로 실행합니다. 실패할 경우 롤백이 가능하며, 이는 데이터의 일관성을 유지하면서 동시성을 관리하는 또 다른 방법입니다.
  - **함수형 프로그래밍(Functional Programming):** 불변성(Immutability)을 강조하여 공유 상태의 변경을 피함으로써 동시성 문제를 자연스럽게 회피할 수 있습니다.

### **13. Thread Pool, Monitor, Fork-Join에 대해 설명해 주세요. ✅**

**Thread Pool:**
Thread Pool은 사전에 생성된 스레드의 집합으로, 작업을 동시에 실행하기 위해 재사용됩니다. 이러한 방식은 스레드 생성과 소멸에 따른 오버헤드를 줄이고, 자원의 효율적인 사용을 가능하게 합니다. 작업이 발생하면, Thread Pool에서 사용 가능한 스레드가 작업을 수행하고, 작업이 완료되면 다시 Pool로 반환되어 다른 작업에 재사용됩니다.

**Monitor:**
Monitor는 동기화 목적으로 사용되는 객체로, 특정 객체에 대한 동시 접근을 제어합니다. Monitor는 일반적으로 락(lock)과 조건 변수(condition variable)를 사용하여 구현됩니다. 하나의 스레드만이 동시에 Monitor로 보호되는 코드 영역(임계 영역)에 접근할 수 있으며, 다른 스레드는 현재 접근 중인 스레드가 영역을 떠날 때까지 대기해야 합니다.

**Fork-Join:**
Fork-Join은 병렬 프로그래밍을 위한 프레임워크 중 하나로, 작업을 여러 개의 작은 하위 작업으로 나눈 뒤(분할 정복 방식), 이를 병렬로 실행하고 결과를 합쳐서 최종 결과를 도출합니다. 이 프레임워크는 대개 재귀적 알고리즘을 병렬화하는 데 유용하며, CPU 코어를 최대한 활용하여 성능을 향상시킬 수 있습니다.

- **Thread Pool을 사용한다고 가정하면, 어떤 기준으로 스레드의 수를 결정할 것인가요?**
  - **하드웨어:** 사용 가능한 CPU 코어의 수를 고려하여, 각 코어가 최대한 활용될 수 있도록 합니다.
  - **작업의 종류:** CPU 집약적(CPU-bound) 작업이라면 코어 수와 비슷하게 설정하는 것이 좋지만, I/O 집약적(I/O-bound) 작업인 경우 더 많은 스레드를 할당할 수 있습니다.
  - **작업의 특성:** 작업이 얼마나 오래 실행되는지, 그리고 얼마나 많은 I/O 대기가 발생하는지에 따라 스레드 수를 조절할 수 있습니다.
- **어떤 데이터를 정렬 하려고 합니다. 어떤 방식의 전략을 사용하는 것이 가장 안전하면서도 좋은 성능을 낼 수 있을까요?**

  - **알고리즘 선택:** 데이터의 크기와 특성에 따라 적절한 정렬 알고리즘을 선택합니다. 예를 들어, 작은 데이터 세트에는 삽입 정렬이 효율적일 수 있으나, 큰 데이터 세트에는 퀵 정렬이나 병합 정렬이 더 적합할 수 있습니다.
  - **병렬 정렬:** 큰 데이터 세트의 경우, 병렬 정렬 알고리즘(예: 병렬 퀵 정렬, 병합 정렬)을 사용하여 여러 프로세서나 코어에서 작업을 분산시키는 것이 좋습니다. 이는 데이터를 더 빠르게 정렬할 수 있게 해줍니다.
  - **안정성:** 안정적인 정렬 알고리즘을 사용하면 동일한 값을 가진 요소들의 상대적 순서가 변경되지 않습니다. 이는 특정 상황에서 중요할 수 있습니다.

### **14. 캐시 메모리 및 메모리 계층성에 대해 설명해 주세요. ✅**

- **캐시 메모리는 어디에 위치해 있나요?**
  - 캐시 메모리는 CPU 와 주 메모리(RAM) 사이에 위치하는 고속 메모리로, SRAM 기반의 저장 장치이다. (SRAM 은 Static RAM으로, 저장된 데이터가 변하지 않는 RAM이다) 캐시 메모리는 CPU 내부 또는 CPU와 가까운 위치에 배치된다. 이는 CPU가 데이터에 더 빠르게 접근할 수 있도록 하기 위함이다.
  - (RAM = 동네 대형 마트, 캐시 메모리 = 집 앞 편의점)
- **L1, L2 캐시에 대해 설명해 주세요.**
  - 코어에 가장 가까운 캐시 메모리를 L1 캐시, 그 다음을 L2 캐시, 그 다음을 L3캐시이다. L1, L2 캐시는 CPU(코어) 내부에 있고, L3 은 코어 외부에 있다.
  - **L1 캐시:** CPU 코어에 가장 가까운 캐시로, 매우 빠른 액세스 속도를 가지나 용량이 작습니다. 일반적으로 명령어 캐시와 데이터 캐시로 구분됩니다.
  - **L2 캐시:** L1 캐시보다 약간 느리지만 더 큰 용량을 가집니다. L1 캐시에서 찾을 수 없는 데이터를 저장하고 있습니다.
  - 멀티코어 프로세서에서는, L1 & L2 캐시는 코어마다 고유한 캐시 메모리로 할당되고, L3 은 여러 코어가 공유하는 형태이다.
- **캐시 히트, 캐시 미스, 캐시 적중률이란 무엇인가요?**
  - 캐시 메모리는 CPU가 사용할 법한 대상을 예측하여 저장한다. 이때 예측이 들어맞아 캐시 메모리 내 데이터가 CPU에서 활용된 경우를 캐시 히트라고 한다.
  - 반대로 예측이 틀린 경우를 캐시 미스라고 한다.
  - 캐시 적릉률 = (캐시 히트 횟수) / (캐시 히트 횟수 + 캐시 미스 횟수)
- **캐시에 올라오는 데이터는 어떻게 관리되나요?**
  - **캐시에 올라오는 데이터는 주로 사용 빈도나 최근 사용 기록에 따라 관리됩니다.**
  - **교체 알고리즘:** LRU (Least Recently Used), FIFO (First In, First Out), LFU (Least Frequently Used) 등의 알고리즘을 사용해 캐시 내용을 관리합니다.
- **캐시간의 동기화는 어떻게 이루어지나요?**
  - 캐시 간 동기화는 캐시 일관성 프로토콜을 사용하여 이루어집니다. 이 프로토콜은 여러 캐시 간의 데이터 일관성을 유지하기 위해 사용되며, 대표적인 예로 MESI 프로토콜이 있습니다.
- **캐시 메모리의 Mapping 방식에 대해 설명해 주세요.**
  1. 직접 매핑(Direct Mapped Cache):
     - 각 메모리 블록이 하나의 캐시 라인에만 매핑됩니다.
  2. 전체 연관 매핑(Fully Associative Cache):
     - 메모리 블록이 캐시의 어느 라인에도 매핑될 수 있습니다.
  3. 집합 연관 매핑(Set Associative Cache):
     - 중간 형태로, 캐시를 여러 집합으로 나누고 각 집합 내에서는 전체 연관 매핑 방식을 사용합니다.
- **캐시의 지역성에 대해 설명해 주세요.**
  - 캐시는 참조 지역성의 원리에 따라 메모리로부터 가져올 데이터를 결정한다.
  - **시간 지역성(Temporal Locality):** 한 번 접근한 데이터는 곧 다시 접근될 가능성이 높습니다. (ex. 프로그램에서 구구단 실행)
  - **공간 지역성(Spatial Locality):** 한 데이터 주변의 데이터가 곧 접근될 가능성이 높습니다.
- **캐시의 지역성을 기반으로, 이차원 배열을 가로/세로로 탐색했을 때의 성능 차이에 대해 설명해 주세요.**
  - 대부분의 프로그래밍 언어에서 이차원 배열은 연속적인 메모리 블록에 행 단위로 저장됩니다. 예를 들어, **`array[3][4]`**가 있다면, **`array[0][0], array[0][1], array[0][2], array[0][3], array[1][0], ...`** 순으로 메모리에 저장됩니다.
  - **가로 탐색:** 가로로 탐색할 때, 즉 행을 따라 연속적으로 데이터를 읽을 때, 메모리에 연속적으로 저장된 데이터를 순서대로 접근합니다. 이 경우 **공간 지역성**이 잘 활용되어, 캐시 히트율이 높아집니다. 캐시에서 데이터를 불러올 때 인접한 데이터도 함께 불러오기 때문에, 다음 접근 시 이미 캐시에 데이터가 존재할 가능성이 높습니다.
  - **세로 탐색:** 세로로 탐색할 때는 행이 바뀔 때마다 큰 메모리 주소의 간격을 건너뛰게 됩니다. 예를 들어, **`array[0][0], array[1][0], array[2][0], ...`** 순으로 접근하게 되는데, 이는 연속적인 메모리 블록을 따라가지 않습니다. 따라서 공간 지역성을 충분히 활용하지 못하며, 캐시 미스가 더 자주 발생할 수 있습니다. 캐시 미스가 발생하면 데이터를 메인 메모리에서 다시 불러와야 하므로, 성능이 저하됩니다.
- **캐시의 공간 지역성은 어떻게 구현될 수 있을까요? (힌트: 캐시는 어떤 단위로 저장되고 관리될까요?)**

  - 캐시의 공간 지역성(Spatial Locality)은 프로그램이 메모리의 인접한 영역을 순차적으로 접근하는 경향을 말합니다. 이를 구현하기 위해, 캐시는 '**블록**' 또는 '**라인**'이라 불리는 단위로 데이터를 저장하고 관리합니다.

  1. **블록 단위로 데이터 로드:**
     - CPU가 특정 메모리 주소를 요청할 때, 해당 주소를 포함하는 전체 캐시 라인(블록)이 캐시 메모리로 로드됩니다.
     - 이는 프로그램이 곧 인접한 메모리 주소에 접근할 것이라는 가정에 기반합니다.
  2. **인접 데이터의 빠른 접근:**
     - 한 번의 메모리 접근으로 여러 데이터를 캐시에 로드함으로써, 인접한 데이터에 대한 후속 접근이 매우 빨라집니다.
     - 예를 들어, 배열을 순차적으로 읽는 경우, 초기 접근 후 인접한 요소들은 이미 캐시에 로드되어 있어 빠르게 접근할 수 있습니다.
  3. **캐시 미스 최소화:**
     - 연속적인 데이터 접근 패턴에서 캐시 미스가 적게 발생합니다. 한 번의 캐시 미스로 인해 여러 데이터가 캐시로 로드되므로, 이후의 접근은 캐시 히트가 될 가능성이 높습니다.

### **15.메모리의 연속할당 방식 세 가지를 설명해주세요. (first-fit, best-fit, worst-fit)**

- **1. First-Fit:**
  First-Fit 방식은 메모리 할당 요청이 들어오면, 메모리의 시작 부분부터 탐색을 시작하여 처음으로 발견된 충분히 큰 공간에 프로세스를 할당하는 방식입니다. 이 방법은 간단하고 실행 속도가 빠르지만, 메모리의 앞부분에 작은 공간들이 많이 생겨 메모리 단편화를 일으킬 수 있습니다.

- **2. Best-Fit:**
  Best-Fit 방식은 할당할 프로세스보다 크거나 같은 모든 빈 공간을 탐색한 후, 가장 작은 충분히 큰 공간에 프로세스를 할당합니다. 이 방법은 메모리를 효율적으로 사용하려는 시도에서 비롯되었으나, 매우 작은 메모리 조각들을 생성할 수 있어 오히려 단편화를 더욱 악화시킬 수 있습니다.

- **3. Worst-Fit:**
  Worst-Fit 방식은 사용 가능한 가장 큰 메모리 공간에 프로세스를 할당하는 방식입니다. 이 방식의 기본 아이디어는 가장 큰 공간을 사용함으로써 남은 공간도 상대적으로 크게 유지되어, 후속 메모리 요청을 수용할 가능성을 높이는 것입니다. 이 방법은 더 큰 공간을 남겨 두기 때문에, 중간 크기의 프로세스 할당 요청을 처리하는 데 유리할 수 있습니다.

- **worst-fit 은 언제 사용할 수 있을까요?**
  - Leaving the biggest leftover hole could be a better approach if all the processes submitted tend to be around the same size.
  - In that case, if you have a large free block (say 50MB) and most of the processes submitted tend to be smaller than that (say 10MB), it would actually be better to put them in this big block so that you can use the leftover space better, rather than put them in a barely bigger block (maybe 11-15MB) which would leave some small unusable space.
  - 만약 프로세스가 비슷한 사이즈(ex. 10MB) 면, 아예 11-15MB 에 할당하는 것보다 50MB 짜리 빈 공간에 할당하는 것이 나을 수 도 있다. 왜냐면 이렇게 하면 남는 공간을 더 효과적으로 사용할 수 있기 때문이다.
- **성능이 가장 좋은 알고리즘은 무엇일까요?**

  - "성능이 가장 좋다"는 것은 메모리 사용의 효율성, 메모리 단편화 최소화, 알고리즘의 실행 시간 등 여러 요소를 고려해야 합니다. 실제 성능은 사용 사례와 시스템의 요구 사항에 따라 다를 수 있습니다.

  - **메모리 사용 효율성**과 **단편화 최소화** 측면에서는 Best-Fit이 이론적으로는 가장 효율적일 수 있습니다. 하지만, Best-Fit은 매우 작은 공간들을 생성할 수 있어, 장기적으로 관리가 어려워질 수 있습니다.
  - **실행 시간** 측면에서는 First-Fit이 일반적으로 빠르게 실행될 수 있습니다. 메모리를 순차적으로 탐색하며 처음으로 충분히 큰 공간을 찾으면 바로 할당하기 때문입니다.
  - Worst-Fit은 메모리 공간을 크게 유지하려는 목적에는 적합할 수 있으나, 가장 큰 공간을 찾는 과정이 오버헤드가 될 수 있습니다.

- **16. Thrashing 이란 무엇인가요?**

  Thrashing은 컴퓨터 시스템에서 발생하는 현상으로, 프로세스가 실행되기 위해 필요한 페이지나 데이터를 메모리에서 찾지 못하여, 지속적으로 페이지 폴트(page fault)를 일으키고, 따라서 페이지를 디스크에서 메모리로 반복적으로 스와핑(swap)하는 상황을 말합니다. 이로 인해 시스템의 CPU 사용률이 실제 유용한 작업에 사용되기보다는 페이지 교체 작업에 소모되어, 시스템의 전반적인 성능이 급격히 저하됩니다. Thrashing은 메모리가 과도하게 할당되어 실제 사용 가능한 메모리보다 많은 양의 작업을 동시에 처리하려 할 때 발생합니다.

  - **Thrashing 발생 시, 어떻게 완화할 수 있을까요?**

    **1. 적절한 Working Set의 조정:**
    Working Set 모델을 사용하여 각 프로세스가 활동적으로 사용하는 페이지 집합의 크기를 조정합니다. 프로세스에 충분한 메모리가 할당되어 있지 않다면, working set의 크기를 증가시켜주어야 합니다. 이는 프로세스가 필요로 하는 페이지를 메모리 내에 유지하도록 도와, 페이지 폴트의 수를 줄입니다.

    **2. 메모리 할당 정책의 최적화:**
    메모리 할당 정책을 조정하여 시스템의 전체 메모리 사용률을 관리합니다. 예를 들어, 메모리 요구가 높은 프로세스의 메모리 할당을 제한하거나, 낮은 우선순위의 프로세스를 스왑 아웃(swap out)하여 고우선순위의 프로세스에 더 많은 메모리를 제공할 수 있습니다.

    **3. 스와핑 전략의 조정:**
    스와핑 전략을 조정하여 시스템의 I/O 부하를 감소시킬 수 있습니다. 예를 들어, 덜 사용되는 페이지부터 스왑 아웃하는 LRU(Least Recently Used) 같은 페이지 교체 알고리즘을 사용하면, 더 중요한 데이터를 메모리 내에 유지할 수 있습니다.

    **4. 멀티 프로그래밍도의 감소:**
    시스템에서 동시에 실행되는 프로세스의 수를 줄이는 것입니다. 멀티 프로그래밍도를 낮추면 각 프로세스에 할당되는 메모리 양이 증가하여 thrashing의 가능성이 감소합니다.

    **5. 자원 할당 및 스케줄링 최적화:**
    자원 할당과 프로세스 스케줄링 알고리즘을 최적화하여, 시스템 자원을 보다 효율적으로 사용하도록 합니다. 이는 프로세스 간의 경쟁을 줄이고, 메모리 사용률을 안정화하는 데 도움이 됩니다.

- **17. 가상 메모리란 무엇인가요?**

  - 가상 메모리는 컴퓨터 시스템의 메모리 관리 기법 중 하나로, 물리 메모리(RAM)의 크기에 제한받지 않고, 프로그램이 더 큰 메모리 공간을 사용할 수 있도록 하는 기술입니다. 이를 통해 시스템은 실제 물리 메모리보다 더 많은 메모리를 가상으로 제공할 수 있으며, 디스크의 일부를 추가 메모리처럼 사용하여 프로세스에 할당합니다. 가상 메모리는 물리 메모리를 **페이지(Page)**라는 작은 단위로 나누고, 이와 동일한 크기의 블록을 디스크 상의 **스왑 공간**에 생성하여 사용합니다.

  - **가상 메모리가 가능한 이유가 무엇일까요?**
    - **페이지 테이블(Page Table):** 가상 주소와 물리 주소 간의 매핑을 관리하는 구조입니다. 이를 통해 시스템은 가상 주소를 실제 메모리의 물리 주소로 변환할 수 있습니다.
    - **디맨드 페이징(Demand Paging):** 프로그램 실행 시 필요한 페이지만을 메모리에 적재하는 기법입니다. 프로그램의 모든 부분을 메모리에 올릴 필요 없이, 실제로 필요한 부분만을 불러오므로 메모리 사용을 최적화할 수 있습니다.
    - **스왑 공간(Swap Space):** 하드 디스크의 일부를 메모리처럼 사용하는 공간입니다. 물리 메모리가 부족할 때, 사용되지 않는 페이지를 스왑 공간으로 이동시켜 메모리를 확보합니다.
  - **Page Fault가 발생했을 때, 어떻게 처리하는지 설명해 주세요.**
    1. **페이지 폴트 감지:** 프로세스가 접근하려는 페이지가 물리 메모리에 없으면, 하드웨어는 페이지 폴트를 감지하고 운영 체제에 신호를 보냅니다.
    2. **인터럽트 처리:** 운영 체제는 페이지 폴트 인터럽트를 받고 현재 실행 중인 프로세스를 중지시킵니다.
    3. **스왑 인(Swap In):** 운영 체제는 필요한 페이지를 디스크의 스왑 영역에서 찾아 물리 메모리로 로드합니다. 만약 물리 메모리에 여유 공간이 없다면, 운영 체제는 하나 이상의 페이지를 스왑 아웃(Swap Out)하여 공간을 확보합니다.
    4. **페이지 테이블 업데이트:** 페이지가 메모리에 로드되면, 운영 체제는 페이지 테이블을 업데이트하여 가상 주소와 물리 주소 간의 새로운 매핑을 생성합니다.
    5. **프로세스 재개:** 모든 처리가 완료되면, 운영 체제는 중단되었던 프로세스의 실행을 재개합니다.
  - **페이지 크기에 대한 Trade-Off를 설명해 주세요.**

    **1. 큰 페이지 크기:**

    - **장점:**
      - 관리 오버헤드 감소: 페이지 테이블의 크기가 줄어들어, 메모리 관리에 필요한 오버헤드가 감소합니다.
      - TLB(Translation Lookaside Buffer) 효율성 증가: TLB는 한 번에 더 많은 메모리 영역을 커버할 수 있어, TLB Miss의 확률이 감소합니다.
    - **단점:**
      - 내부 단편화 증가: 페이지의 일부만 사용되는 경우, 남는 공간은 낭비되므로 메모리 이용 효율이 감소할 수 있습니다.
      - 페이지 교체 시 더 많은 데이터 이동: 페이지 폴트가 발생할 경우, 더 큰 데이터 블록을 메모리와 디스크 간에 이동해야 하므로 I/O 부하가 증가합니다.

    **2. 작은 페이지 크기:**

    - **장점:**[]
      - 내부 단편화 감소: 페이지 크기가 작아 내부 단편화의 양이 줄어들어, 메모리 사용 효율이 증가합니다.
      - 필요한 데이터만 로드: 필요한 데이터만 메모리에 로드할 수 있어, I/O 부하와 페이지 교체 비용이 감소할 수 있습니다.
    - **단점:**
      - 관리 오버헤드 증가: 더 많은 페이지를 관리해야 하므로, 페이지 테이블이 커지고 관리에 필요한 오버헤드가 증가합니다.
      - TLB Miss 증가: 더 많은 페이지 번호와 주소 변환 정보를 저장해야 하므로, TLB Miss가 더 자주 발생할 수 있습니다.

  - **페이지 크기가 커지면, 페이지 폴트가 더 많이 발생한다고 할 수 있나요?**
    페이지 크기와 페이지 폴트 발생률 사이에는 직접적인 상관관계가 없습니다. 페이지 크기가 커지면 단일 페이지 폴트로 인해 더 많은 데이터가 메모리로 로드되므로, 이론적으로 프로그램 실행 도중 필요한 데이터가 이미 메모리에 존재할 확률이 높아집니다. 하지만, 이는 프로그램의 액세스 패턴과 데이터의 지역성에 크게 의존합니다. 큰 페이지는 내부 단편화를 증가시킬 수 있으며, 실제로 필요하지 않은 데이터까지 메모리에 로드될 수 있습니다.
  - **세그멘테이션 방식을 사용하고 있다면, 가상 메모리를 사용할 수 없을까요?**
    세그멘테이션 방식을 사용하는 시스템에서도 가상 메모리를 사용할 수 있습니다. 세그멘테이션은 메모리를 논리적인 단위인 세그먼트로 분할하여 관리하는 기법으로, 각 세그먼트는 다른 크기를 가질 수 있습니다. 세그멘테이션과 가상 메모리는 서로 다른 메모리 관리 전략을 제공하지만, 둘을 결합하여 사용할 수 있습니다. 예를 들어, 세그먼트 내에서 페이지화를 적용하여 세그먼트의 각 부분을 가상 페이지로 관리할 수 있습니다. 이러한 결합은 세그멘테이션의 유연성과 가상 메모리의 효율성을 동시에 활용할 수 있는 장점을 제공합니다. 따라서, 세그멘테이션 방식을 사용하면서도 가상 메모리 기법을 적용하여 프로그램의 주소 공간을 효율적으로 관리하고, 메모리를 보다 유연하게 사용할 수 있습니다.
  - **가상 메모리를 사용하는 이유가 무엇일까요 ? 스왑 말고 더 중요한 이유는 어떤 것이 있을까요?**

### **18. 세그멘테이션과 페이징의 차이점은 무엇인가요?**

- **페이지와 프레임의 차이에 대해 설명해 주세요.**
- **내부 단편화와, 외부 단편화에 대해 설명해 주세요.**
- **페이지에서 실제 주소를 어떻게 가져올 수 있는지 설명해 주세요.**
- **어떤 주소공간이 있을 때, 이 공간이 수정 가능한지 확인할 수 있는 방법이 있나요?**
- **32비트에서, 페이지의 크기가 1kb 이라면 페이지 테이블의 최대 크기는 몇 개일까요?**
- **32비트 운영체제는 램을 최대 4G 까지 사용할 수 있습니다. 이 이유를 페이징과 연관 지어서 설명해 주세요.**
- **C/C++ 개발을 하게 되면 Segmentation Fault 라는 에러를 접할 수 있을텐데, 이 에러는 세그멘테이션/페이징과 어떤 관계가 있을까요?**

### **19. TLB는 무엇인가요?**

- **TLB를 쓰면 왜 빨라지나요?**
- **MMU가 무엇인가요?**
- **TLB와 MMU는 어디에 위치해 있나요?**
- **코어가 여러개라면, TLB는 어떻게 동기화 할 수 있을까요?**
- **TLB 관점에서, Context Switching 발생 시 어떤 변화가 발생하는지 설명해 주세요.**

### **20. 동기화를 구현하기 위한 하드웨어적인 해결 방법에 대해 설명해 주세요.**

- **volatile 키워드는 어떤 의미가 있나요?**
- **싱글코어가 아니라 멀티코어라면, 어떻게 동기화가 이뤄질까요?**

### **21. 페이지 교체 알고리즘에 대해 설명해 주세요.**

- **LRU 알고리즘은 어떤 특성을 이용한 알고리즘이라고 할 수 있을까요?**
- **LRU 알고리즘을 구현한다면, 어떻게 구현할 수 있을까요?**
- **LRU 알고리즘의 단점을 설명해 주세요. 이를 해결할 수 있는 대안에 대해서도 설명해 주세요.**

### **22. File Descriptor와, File System에 에 대해 설명해 주세요.**

- **I-Node가 무엇인가요?**
- **프로그래밍 언어 상에서 제공하는 파일 관련 함수 (Python - open(), Java - BufferedReader/Writer 등)은, 파일을 어떤 방식으로 읽어들이나요?** ###**23. 동기와 비동기, 블로킹과 논블로킹의 차이에 대해 설명해 주세요.**
